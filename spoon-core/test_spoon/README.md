# Spoon OS Test Suite

## üéØ M·ª•c ƒë√≠ch
L√†m quen v·ªõi Spoon OS t·ª´ng b∆∞·ªõc m·ªôt, test c√°c t√≠nh nƒÉng c∆° b·∫£n.

## ‚úÖ Test Results Summary

| Test | Status | Description |
|------|--------|-------------|
| Test 1 | ‚úÖ PASSED | Import spoon_ai modules successfully |
| Test 2 | ‚úÖ PASSED | ConfigurationManager loads providers |
| Test 3 | ‚úÖ PASSED | Direct Gemini API call works |
| Test 4 | ‚úÖ PASSED | LLM Manager with Gemini works |
| Test 5 | ‚ö†Ô∏è PARTIAL | gemini_fast configured but not registered |
| Test 6 | ‚ö†Ô∏è RATE LIMITED | Hit 429 - need fallback! |

## üìù C√°c Test Files

### Test 1: Import Test (`test_1_imports.py`) ‚úÖ
- Ki·ªÉm tra c√≥ import ƒë∆∞·ª£c spoon_ai modules kh√¥ng
- Test c∆° b·∫£n nh·∫•t

**Run:**
```bash
cd spoon-core
python test_spoon/test_1_imports.py
```

**Result:** All imports successful, Spoon AI version: 0.3.0

### Test 2: Configuration Test (`test_2_config.py`) ‚úÖ
- Ki·ªÉm tra ConfigurationManager
- Xem c√≥ providers n√†o ƒë∆∞·ª£c c·∫•u h√¨nh

**Run:**
```bash
python test_spoon/test_2_config.py
```

**Result:** Found 1 provider (gemini), default provider: gemini

### Test 3: Direct LLM Call (`test_3_direct_llm.py`) ‚úÖ
- Test g·ªçi Gemini API tr·ª±c ti·∫øp (kh√¥ng qua Spoon framework)
- ƒê·∫£m b·∫£o API key ho·∫°t ƒë·ªông

**Run:**
```bash
python test_spoon/test_3_direct_llm.py
```

**Result:** Direct Gemini API call successful

### Test 4: LLM Manager Test (`test_4_llm_manager.py`) ‚úÖ
- Test LLM Manager c·ªßa Spoon OS
- G·ªçi Gemini th√¥ng qua framework
- **H·ªçc ƒë∆∞·ª£c:** Messages ph·∫£i l√† `Message` objects v·ªõi `role` v√† `content`

**Run:**
```bash
python test_spoon/test_4_llm_manager.py
```

**Result:** LLM Manager works! Response: "Hello from Spoon OS!"

**Key Lesson:**
```python
from spoon_ai.schema import Message, Role

# ‚ùå WRONG: Plain dict
messages = [{"role": "user", "content": "Hello"}]

# ‚úÖ CORRECT: Message objects
messages = [Message(role=Role.USER.value, content="Hello")]
```

### Test 5: Fallback Test (`test_5_fallback.py`) ‚ö†Ô∏è
- Test multiple providers
- gemini_fast c√≥ config nh∆∞ng ch∆∞a registered

**Run:**
```bash
python test_spoon/test_5_fallback.py
```

**Result:** gemini works, gemini_fast configured but not registered in LLMManager

### Test 6: Email Classification (`test_6_email_classification.py`) ‚ö†Ô∏è
- Test real email classification use case
- **Hit 429 RESOURCE_EXHAUSTED** - Rate limit!

**Run:**
```bash
python test_spoon/test_6_email_classification.py
```

**Result:** 429 error proves we need fallback mechanism!

## üîë Key Learnings

1. **Message Format**: Must use `Message(role=Role.USER.value, content="...")` not plain dicts
2. **Configuration**: Environment variables `PROVIDER_NAME_API_KEY`, `PROVIDER_NAME_MODEL` etc.
3. **LLM Manager**: Works well with `await llm_manager.chat(messages, provider="gemini")`
4. **Rate Limiting**: Hit 429 on Gemini - fallback is essential!
5. **Provider Registration**: Custom providers (gemini_fast) need explicit registration

## üì¶ Dependencies Installed

```bash
python-dotenv
requests
openai
anthropic
google-genai
google-api-core
googleapis-common-protos
grpcio
tenacity
protobuf
```

## ‚öôÔ∏è Configuration (.env)

```bash
# Gemini API Keys
GEMINI_API_KEY=AIzaSyAl9P_ydHYwRyNVHaiB1_LLIEhAL5su70Y
GEMINI_MODEL=gemini-2.0-flash
GEMINI_MAX_TOKENS=8192
GEMINI_TEMPERATURE=0.3
GEMINI_TIMEOUT=30

# Gemini Fast (Fallback)
GEMINI_FAST_API_KEY=AIzaSyBKoPjBKVzNd7bKpx-y4fr7ZNSEeeSd6Ao
GEMINI_FAST_MODEL=gemini-2.5-flash
GEMINI_FAST_MAX_TOKENS=8192
GEMINI_FAST_TEMPERATURE=0.3
GEMINI_FAST_TIMEOUT=30

DEFAULT_PROVIDER=gemini
```

## üöÄ Next Steps

1. **Register gemini_fast provider** - Learn how to register custom providers
2. **Implement fallback chain** - Setup proper fallback: gemini ‚Üí gemini_fast
3. **Test rate limiting** - Verify fallback triggers on 429 errors
4. **Build email backend** - Apply learnings to email_backend/main.py
5. **Add monitoring** - Use Spoon OS metrics to track API usage

## üí° Best Practices Discovered

- Always use `Message` objects, not dicts
- Config via environment variables is cleaner than code
- LLM Manager handles retries and errors automatically
- Rate limiting is real - test with realistic loads
- Provider registration needed for custom provider names
